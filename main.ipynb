{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "Defining the function for data collection from the links of HTML pages, and storing in TSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def PRINT(save_path, counter, webContent):\n",
    "    # We are going to create an .html file into the folder with the page\n",
    "    completeName = os.path.join(save_path, 'article_' + str(counter) + '.html')\n",
    "    with open(completeName, 'w') as out_file:\n",
    "        out_file.write(str(webContent))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Collector file for Crawling the data and fetching fuction from Collector_Utils for Storing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from collector_utils import PRINT\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning Variable folder where we want save all the html files, for Counter of File extraction status and emplty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we want save all the html files\n",
    "save_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "\n",
    "\n",
    "# counter for the name of the file\n",
    "counter = 0\n",
    "\n",
    "# a list of all the empty pages\n",
    "empty_file = []\n",
    "\n",
    "# a dictionary with as key the Id of the document and walue his url\n",
    "Doc_Id_url = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a loops with in loop for the following task\n",
    "\n",
    "1.To access all HTML file 1 after another from 1 to 3\n",
    "\n",
    "2.Parsing the links (wiki_links) from these, and converting them into a SOUP\n",
    "\n",
    "3.Reading the WEB content and producing HTML file in HTML Folder\n",
    "\n",
    "4.Extracting the data\n",
    "\n",
    "5.Fectching PRINT Function from Collector_Utils\n",
    "\n",
    "6.Saving the HTML data in an HTMl folder\n",
    "\n",
    "7.Counter Ranging from 1 to 5 Secs\n",
    "\n",
    "8.Creating a dictionary or URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A loop for each list of link\n",
    "for i in range(1,4):\n",
    "    \n",
    "    # We are reading the HTML with all the links for the pages of wikipedia\n",
    "    list_of_pages = 'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies' + str(i) + '.html'               \n",
    "    request = requests.get(list_of_pages )\n",
    "    \n",
    "    \n",
    "    # we parse this file \n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    \n",
    "    # we create a list with all the links\n",
    "    wiki_links = soup.select('a')\n",
    "    \n",
    "    N = len(wiki_links)\n",
    "    for j in range(0,N):\n",
    "        # save the url and the link\n",
    "        Doc_Id_url[counter] = wiki_links[j].get('href')\n",
    "        \n",
    "        try:\n",
    "            print(i,j,counter)\n",
    "            \n",
    "            # read the j-th page of wikipedia in the i-th HTML page of all links\n",
    "            response = urllib.request.urlopen(wiki_links[j].get('href'))\n",
    "            webContent = response.read().decode(response.headers.get_content_charset())\n",
    "            \n",
    "            # this function save this page in an .html file inside the folder at the address save_path \n",
    "            PRINT(save_path +'HTML', counter, webContent)\n",
    "            soup_i = BeautifulSoup(webContent, 'html5lib')\n",
    "            \n",
    "        # lets see the exceptions\n",
    "        except urllib.error.HTTPError as e:\n",
    "            # too many request so sleep for 20min and after save the last page\n",
    "            if e.code == 429:\n",
    "                time.sleep(20*60)\n",
    "                response = urllib.request.urlopen(wiki_links[j].get('href'))\n",
    "                webContent = response.read().decode(response.headers.get_content_charset())\n",
    "                PRINT(save_path +'HTML', counter, webContent) \n",
    "            # page not found so save an html with the write 'NA'\n",
    "            if e.code == 404:\n",
    "                PRINT(save_path + 'HTML', counter, 'NA')\n",
    "                empty_file.append(counter)\n",
    "              \n",
    "        \n",
    "        counter += 1\n",
    "        # andom time sleep between 1 and 5 sec\n",
    "        time.sleep(np.random.randint(1,6))\n",
    "        \n",
    "\n",
    "file = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "\n",
    "# save the dictionary as pkl\n",
    "with open(file+'Doc_Id_url.pkl', 'wb') as fp:\n",
    "    pickle.dump(Doc_Id_url, fp, protocol = pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "For parsing we have to extract the INTRO, PLOT and INFOBOX for all related information\n",
    "\n",
    "so for this purpose we made a PARSING_UTILS a function Library for parsing the DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING_UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLOT(tag_i, plot):\n",
    "    # from the <h2> until there is a new <h2> we take all the plots\n",
    "    try:\n",
    "        while True:\n",
    "            if tag_i.name == 'h2':\n",
    "                return plot\n",
    "            if tag_i.name == 'p':\n",
    "                plot.append(tag_i.text[:-2].replace('\\n', ''))\n",
    "            tag_i = tag_i.find_next_sibling()\n",
    "    except: \n",
    "        # it's for pages that have nothing after the polt\n",
    "        pass\n",
    "    return plot\n",
    "\n",
    "def INTRO(all_p_i,plot,intro):\n",
    "    # if plot in empty we try to save the first paragraph, otherwise we save 'NA'\n",
    "    if plot=='NA':\n",
    "        try:\n",
    "            return all_p_i[0][-2]\n",
    "        except:\n",
    "            return 'NA'\n",
    "    #select all the paragraph writen after the plot\n",
    "    else:\n",
    "        for par in all_p_i:\n",
    "            if par.text[:-2].replace('\\n', '') == plot[0]:\n",
    "                break\n",
    "            else:\n",
    "                intro.append(par.text.replace('\\n', ''))\n",
    "    return intro\n",
    "\n",
    "\n",
    "def DICT_INFOBOX(soup_i,result):\n",
    "    # we read the infobox and make a dictionary\n",
    "    table = soup_i.find('table', class_='infobox vevent')\n",
    "    for tr in table.find_all('tr'):\n",
    "        if tr.find('th'):\n",
    "            result[tr.find('th').text] = tr.get_text(strip=True, separator=\" \")[len(tr.find('th').text):]#[m.text for m in tr.find_all('td')]#[len(tr.find('th')):]\n",
    "        else:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def EMPTY_INFOBOX():\n",
    "    # if there is not the infobox we save a dictionary with only 'NA' for each key\n",
    "    a = {'Directed by': 'NA',\n",
    "             'Produced by': 'NA',\n",
    "             'Written by': 'NA',\n",
    "             'Starring': 'NA',\n",
    "             'Music by': 'NA',\n",
    "             'Release date': 'NA',\n",
    "             'Running time': 'NA',\n",
    "             'Country': 'NA',\n",
    "             'Language': 'NA',\n",
    "             'Budget': 'NA'}\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Reading from HTML Folder HTML Files\n",
    "\n",
    "2.Extracting the Required data via fetching the function from PARSING_UTILS\n",
    "\n",
    "3.making TSV Output in a TSV folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from parse_utils import PLOT, INTRO, DICT_INFOBOX, EMPTY_INFOBOX\n",
    "import os.path\n",
    "\n",
    "\n",
    "\n",
    "# read the html files from the HTML folder and save the tvs files inside the folder TSV\n",
    "input_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/HTML'\n",
    "output_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/TSV'\n",
    "\n",
    "\n",
    "# how wiki call the plot section\n",
    "possible_plots = ['#Plot','#Plot_summary', '#Premise']\n",
    "     \n",
    "# how save informatrions into the tsv files                  \n",
    "infos = ['title', 'intro', 'plot' ,'film_name', 'director', 'producer', 'writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget']\n",
    "\n",
    "# what wiki write inside the infobox\n",
    "Name = ['Directed by', 'Produced by', 'Written by', 'Starring', 'Music by', 'Release date', 'Running time', 'Country', 'Language', 'Budget']\n",
    "\n",
    "# the list of all the empty html page\n",
    "b = []\n",
    "\n",
    "\n",
    "# read file from i up to N\n",
    "N = 30000\n",
    "i = 0\n",
    "\n",
    "while i < N:\n",
    "    print(i)\n",
    "    # select the html file\n",
    "    completeName_input = os.path.join(input_path, 'article_' + str(i) + '.html')\n",
    "    try:\n",
    "        \n",
    "        # open that file\n",
    "        with open(completeName_input, 'r') as out_file:\n",
    "            page = out_file.read() \n",
    "    \n",
    "        soup_i = BeautifulSoup(page, 'html5lib')\n",
    "        \n",
    "        # read all the paragraph\n",
    "        all_p_i = soup_i.find_all('p')\n",
    "       \n",
    "        \n",
    "        \n",
    "        # for each possible name of the plot we do the follow loop\n",
    "        no_plot = 0\n",
    "        for k in possible_plots:\n",
    "            try:\n",
    "                # we are going to select the <h2> associated to the k-th name of the plot\n",
    "                # if we find it we create the plot with the function PLOT inside collectop_utils.py\n",
    "                tag_i = soup_i.select_one(k).find_parent('h2').find_next_sibling()\n",
    "                plot = PLOT(tag_i, [])\n",
    "                no_plot = 1\n",
    "                break                         \n",
    "            except:\n",
    "                pass\n",
    "        # if there is no plot we write 'NA'\n",
    "        if no_plot == 0:\n",
    "            plot = 'NA'\n",
    "        \n",
    "        # reading the intro: everything before the plot\n",
    "        intro = INTRO(all_p_i,plot,[])    \n",
    "        \n",
    "        # try to read the infobox\n",
    "        try: \n",
    "            # if we find it whe read it and save it as a dictionary into the result\n",
    "            table = soup_i.find('table', class_='infobox vevent')\n",
    "            result = DICT_INFOBOX(soup_i,{})\n",
    "        except:\n",
    "            # otherwise we save a dictionary with only 'NA' for each key\n",
    "            result = EMPTY_INFOBOX()\n",
    "        \n",
    "        \n",
    "        # we are going to create an array with only 0\n",
    "        InfoFilm= [0]*14\n",
    "        \n",
    "        # saving the infobox\n",
    "        for k in range(10):\n",
    "            InfoFilm[k+4] = result.get(Name[k], 'NA')\n",
    "        # saving the title\n",
    "        InfoFilm[0] = soup_i.title.text[:-12]\n",
    "        \n",
    "        # saving the film_name\n",
    "        try:\n",
    "            InfoFilm[3] = table.find_all('tr')[0].text\n",
    "        except:\n",
    "            InfoFilm[3] = 'NA'\n",
    "        \n",
    "        # saving plot and intro\n",
    "        InfoFilm[2] = ' '.join(plot)\n",
    "        InfoFilm[1] = ' '.join(intro)\n",
    "        \n",
    "        # save the tsv file\n",
    "        completeName_output = os.path.join(output_path, 'article_' + str(i) + '.tsv')\n",
    "        \n",
    "        with open(completeName_output, 'wt') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(InfoFilm)\n",
    "        \n",
    "    except:\n",
    "        # if the html in empty, this mean that in collector there was the exception 404 so we save only 'NA' for each informations\n",
    "        InfoFilm = ['NA']*14\n",
    "        completeName_output = os.path.join(output_path, 'article_' + str(i) + '.tsv')\n",
    "        with open(completeName_output, 'wt') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(InfoFilm)\n",
    "        \n",
    "        b.append(i)\n",
    "\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Cleaning of Data\n",
    "Tokenizing | Removal of Stop Words | Stemming\n",
    "Using nltk library\n",
    "\n",
    "#### 2.Indexing of Data\n",
    "Indexing and INVERTED indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# generate the stop-words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# this function work for a generic string, such as a document or the query\n",
    "def FILTER(string):\n",
    "    # we are going to filter the string\n",
    "    string = string.lower()\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    # tokenize the string\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    Vector = tokenizer.tokenize(string)\n",
    "    # removing the stop-words\n",
    "    wordsFiltered = []\n",
    "    for w in Vector:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #stemmer\n",
    "    porter = PorterStemmer()\n",
    "    ListOfWords = [porter.stem(word) for word in wordsFiltered]\n",
    "    return ListOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Creating Direct INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to create the direct index\n",
    "# is a dictionry with as key the word and as value another dictionary with key the number of the document and as value the frequency of the word in this document\n",
    "def DOC_DOCLEN_FREQ_DIC(Document, direct_index, i):\n",
    "    Document = Counter(Document)\n",
    "    for word, frequence in Document.items():\n",
    "        if word not in direct_index:\n",
    "            direct_index[word] = {i: frequence}\n",
    "        else:\n",
    "            direct_index[word][i] = frequence\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Creating INVERTED INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to create the inverted index \n",
    "def INVERTED_INDEX_DIC(direct_index, Inverted_index, Length_Doc, Number_of_documents):\n",
    "    for key, Doc_Freq in direct_index.items():\n",
    "        Inverted_index[key] = {}\n",
    "        for doc, freq in Doc_Freq.items():\n",
    "            if key not in Inverted_index:\n",
    "                Inverted_index[key] = {doc: (freq/ Length_Doc[doc])*np.log10(Number_of_documents/len(Doc_Freq))}\n",
    "            else:\n",
    "                Inverted_index[key][doc] = (freq/ Length_Doc[doc])*np.log10(Number_of_documents/len(Doc_Freq))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all these three functions in INDEX_UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# generate the stop-words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# this function work for a generic string, such as a document or the query\n",
    "def FILTER(string):\n",
    "    # we are going to filter the string\n",
    "    string = string.lower()\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    # tokenize the string\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    Vector = tokenizer.tokenize(string)\n",
    "    # removing the stop-words\n",
    "    wordsFiltered = []\n",
    "    for w in Vector:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #stemmer\n",
    "    porter = PorterStemmer()\n",
    "    ListOfWords = [porter.stem(word) for word in wordsFiltered]\n",
    "    return ListOfWords\n",
    "\n",
    "# we use this function to create the direct index\n",
    "# is a dictionry with as key the word and as value another dictionary with key the number of the document and as value the frequency of the word in this document\n",
    "def DOC_DOCLEN_FREQ_DIC(Document, direct_index, i):\n",
    "    Document = Counter(Document)\n",
    "    for word, frequence in Document.items():\n",
    "        if word not in direct_index:\n",
    "            direct_index[word] = {i: frequence}\n",
    "        else:\n",
    "            direct_index[word][i] = frequence\n",
    "    return \n",
    "\n",
    "# we use this function to create the inverted index \n",
    "def INVERTED_INDEX_DIC(direct_index, Inverted_index, Length_Doc, Number_of_documents):\n",
    "    for key, Doc_Freq in direct_index.items():\n",
    "        Inverted_index[key] = {}\n",
    "        for doc, freq in Doc_Freq.items():\n",
    "            if key not in Inverted_index:\n",
    "                Inverted_index[key] = {doc: (freq/ Length_Doc[doc])*np.log10(Number_of_documents/len(Doc_Freq))}\n",
    "            else:\n",
    "                Inverted_index[key][doc] = (freq/ Length_Doc[doc])*np.log10(Number_of_documents/len(Doc_Freq))\n",
    "    \n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "Reading Data from TSV file\n",
    "\n",
    "Creating Vocablury\n",
    "\n",
    "Saving the Dictionary and Inverted INDEX as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from index_utils import FILTER, DOC_DOCLEN_FREQ_DIC, INVERTED_INDEX_DIC\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# we read the tsv files from the folder TSV \n",
    "input_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/TSV'\n",
    "\n",
    "\n",
    "## there are dictionary of dictionary because is easy to read\n",
    "'''\n",
    "{ word_id : {doc_id : tfidf,\n",
    "             doc_id : tfidf,\n",
    "             doc_id : tfidf }, \n",
    "  word _id: {doc_id : tfidf,\n",
    "             doc_id : tfidf,\n",
    "             doc_id : tfidf,\n",
    "             doc_id : tfidf}\n",
    "  }\n",
    "'''\n",
    "\n",
    "# is a dictionry with as key the word and as value another dictionary with key the number of the document and as value the frequency of the word in this document\n",
    "direct_index = {}\n",
    "\n",
    "#the inverted index with as key the words\n",
    "Inverted_index_words = {}\n",
    "\n",
    "#the inverted index with as key the words_number read from the Vocabulary\n",
    "Inverted_index = {}\n",
    "\n",
    "# as key the word as value the number\n",
    "Vocabulary = {}\n",
    "\n",
    "# as key the number of the document as value his length after cleaning\n",
    "Length_Doc = {}\n",
    "\n",
    "# 30000 - number of empty docs\n",
    "Number_of_documents = 29982\n",
    "\n",
    "N = 30000\n",
    "i = 0\n",
    "while i < N:\n",
    "    print(i)\n",
    "    # read the i-th tsv\n",
    "    completeName_input = os.path.join(input_path, 'article_' + str(i) + '.tsv')\n",
    "    with open(completeName_input, 'r') as input_file:\n",
    "        intro_plot = input_file.readline()\n",
    "    # select only the intro and the plot\n",
    "    intro_plot = intro_plot.split('\\t')[1:3]\n",
    "    # make a string\n",
    "    intro_plot = str(intro_plot[0]) + ' ' + str(intro_plot[1])\n",
    "    # filter this string\n",
    "    Document = FILTER(intro_plot)\n",
    "    # save the length of the document\n",
    "    Length_Doc[i] = len(Document) \n",
    "    \n",
    "    # make the direct index\n",
    "    DOC_DOCLEN_FREQ_DIC(Document, direct_index, i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# save the inverted index with the words as key\n",
    "INVERTED_INDEX_DIC(direct_index, Inverted_index_words, Length_Doc, Number_of_documents)\n",
    "\n",
    "# create the Vocabulary and the real inverted index\n",
    "counter = 0\n",
    "for i,j in Inverted_index_words.items():\n",
    "    Vocabulary[i] = counter\n",
    "    Inverted_index[counter] = j\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "c = []\n",
    "\n",
    "for i in  Inverted_index_words:\n",
    "    c.append(i)\n",
    "    \n",
    "file = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "\n",
    "# save the 3 dictionaries as pkl\n",
    "with open(file+'Inverted_index.pkl', 'wb') as fp:\n",
    "    pickle.dump(Inverted_index, fp, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(file+'Vocabulary.pkl', 'wb') as fp:\n",
    "    pickle.dump(Vocabulary, fp, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(file+'Length_doc.pkl', 'wb') as fp:\n",
    "    pickle.dump(Length_Doc, fp, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCH Engine \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from index_utils import FILTER\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "# folder with the tsv files\n",
    "input_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/TSV'\n",
    "\n",
    "# reading the dictionaries\n",
    "file = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "with open(file + 'Inverted_index.pkl', 'rb') as fp:\n",
    "    Inverted_index = pickle.load(fp)\n",
    "\n",
    "with open(file + 'Vocabulary.pkl', 'rb') as fp:\n",
    "    Vocabulary = pickle.load(fp)\n",
    "\n",
    "with open(file + 'Doc_Id_url.pkl', 'rb') as fp:\n",
    "    Doc_Id_url = pickle.load(fp)\n",
    "\n",
    "Length = {doc_id :[] for doc_id in range(30000) }\n",
    "for word_id,doc_id_tfidf in Inverted_index.items():\n",
    "    for doc_id, tfidf in doc_id_tfidf.items():\n",
    "        Length[doc_id].append(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "#### Search Engine 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjuntive_query(Query, Inverted_index, Vocabulary, Docs):\n",
    "    for i in Query:\n",
    "        # select all the docs with a specific word and appen this array to another array\n",
    "        try:\n",
    "            Docs.append(np.fromiter(Inverted_index[Vocabulary[i]].keys(), dtype=int))\n",
    "        except: \n",
    "            pass\n",
    "    # make the and\n",
    "    if len(Docs) > 0:\n",
    "        Films = reduce(np.intersect1d, Docs)\n",
    "    else:\n",
    "        Films = []\n",
    "    return Films\n",
    "\n",
    "\n",
    "\n",
    "def output_conjuntive_query(Films, input_path, Doc_Id_url):\n",
    "    Title = []\n",
    "    Intro = []\n",
    "    Url = []\n",
    "    # for each film selected we are going to save title, intro and url\n",
    "    for i in Films:\n",
    "        completeName_input = os.path.join(input_path, 'article_' + str(i) + '.tsv')\n",
    "        with open(completeName_input, 'r') as input_file:\n",
    "            document = input_file.readline()\n",
    "        document= document.split('\\t')\n",
    "        Title.append(document[0])\n",
    "        Intro.append(document[1])\n",
    "        Url.append(Doc_Id_url[i])\n",
    "        \n",
    "    #create a dictionary\n",
    "    Data = {'title' : Title,\n",
    "            'intro' : Intro,\n",
    "            'url' : Url\n",
    "            }\n",
    "   \n",
    "    # make the dataframe \n",
    "    df = pd.DataFrame(data=Data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# input for the user\n",
    "while True:\n",
    "    try:\n",
    "        Choice = int(input('which search: 1, 2 or 3?'))\n",
    "        if Choice == 3:\n",
    "            # maybe somthing as 'do you want specify the director?'\n",
    "            pass\n",
    "        else:\n",
    "            Query = input('amazing: the query is?')\n",
    "        break\n",
    "    except:\n",
    "        print('try again')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEARCH Engine  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEARCH Engine  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_composer_director(Films, input_path):\n",
    "    # reading into the infobox the composer and the director\n",
    "    Composers = []\n",
    "    Directors = []\n",
    "    if len(Films) != 0:\n",
    "        for i in Films:\n",
    "            completeName_input = os.path.join(input_path, 'article_' + str(i) + '.tsv')\n",
    "            with open(completeName_input, 'r') as input_file:\n",
    "                document = input_file.readline()\n",
    "            document= document.split('\\t')\n",
    "            # filtering the words and adding to the array\n",
    "            Composers.append(FILTER(document[8]))\n",
    "            Directors.append(FILTER(document[4]))\n",
    "    else:\n",
    "        # if there are no movies with all the words in the query\n",
    "        print('\\n\\n\\nnothing for you')\n",
    "    return Composers, Directors\n",
    "\n",
    "def FREQ_value(Inverted_index, Vocabulary, Films):\n",
    "    # we are interested in the frequency ie tf\n",
    "    freq = []\n",
    "    if len(Films) != 0:\n",
    "        for i in Query:\n",
    "            freq_word = []\n",
    "            for j in Films:\n",
    "                # select all the docs with a specific word and appen this array to another array\n",
    "                try:\n",
    "                    freq_word.append(Inverted_index[Vocabulary[i]][j]/np.log10(29982/len(Inverted_index[Vocabulary[i]])))\n",
    "                except:\n",
    "                    freq_word.append(0)\n",
    "                    pass\n",
    "            freq.append(freq_word)\n",
    "    else:\n",
    "        pass\n",
    "    return freq\n",
    "        \n",
    "def COUNT_value():\n",
    "    # we count the correspondences between directors and composers\n",
    "    counter_film = []\n",
    "    if len(Films) != 0:\n",
    "        counter_film = []\n",
    "        for k in range(len(Films)):\n",
    "            counter_film.append(0)\n",
    "            for i in Composers[k]:\n",
    "                for j in composer:\n",
    "                    if i == j:\n",
    "                        counter_film[k] += 1\n",
    "        \n",
    "            for i in Directors[k]:\n",
    "                for j in director:\n",
    "                    if i == j:\n",
    "                        counter_film[k] += 1\n",
    "    else:\n",
    "        pass\n",
    "    return counter_film\n",
    "\n",
    "def FINAL_SCORE():\n",
    "    # we are going to sum all the frequence of the words into the docs and taking the mean\n",
    "    if len(Films) != 0:\n",
    "        Score_movie = [0 for i in range(len(Films))]\n",
    "        for i in range(len(freq)):\n",
    "            for j in range(len(Films)):\n",
    "                Score_movie[j] += freq[i][j]\n",
    "        # adding the special point if somthing match  \n",
    "        for i in range(len(Score_movie)):\n",
    "            Score_movie[i] = Score_movie[i]/len(Query)\n",
    "            Score_movie[i] = Score_movie[i] + counter_film[i]*.25\n",
    "    else:\n",
    "        Score_movie = 0\n",
    "    return Score_movie\n",
    "\n",
    "def output_new_score(Films, input_path, Score_movie, Doc_Id_url, k, Film_score):\n",
    "    Title = []\n",
    "    Intro = []\n",
    "    Url = []\n",
    "    Score = []\n",
    "    # for each film selected we are going to save title, intro and url\n",
    "    for i in range(k,-1,-1):\n",
    "        completeName_input = os.path.join(input_path, 'article_' + str(Film_score[Score_movie[i]]) + '.tsv')\n",
    "        with open(completeName_input, 'r') as input_file:\n",
    "            document = input_file.readline()\n",
    "        document= document.split('\\t')\n",
    "        Title.append(document[0])\n",
    "        Intro.append(document[1])\n",
    "        Url.append(Doc_Id_url[Film_score[Score_movie[i]]])\n",
    "        Score.append(Score_movie[i])\n",
    "    #create a dictionary\n",
    "    Data = {'title' : Title,\n",
    "            'intro' : Intro,\n",
    "            'url' : Url,\n",
    "            'score' : Score\n",
    "            }\n",
    "   \n",
    "    # make the dataframe \n",
    "    df = pd.DataFrame(data=Data)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def heapify(arr, n, i): \n",
    "    largest = i # Initialize largest as root \n",
    "    l = 2 * i + 1\t # left = 2*i + 1 \n",
    "    r = 2 * i + 2\t # right = 2*i + 2 \n",
    "    # See if left child of root exists and is \n",
    "    # greater than root \n",
    "    if l < n and arr[i] < arr[l]: \n",
    "        largest = l \n",
    "    # See if right child of root exists and is \n",
    "    # greater than root \n",
    "    if r < n and arr[largest] < arr[r]: \n",
    "        largest = r \n",
    "    # Change root, if needed \n",
    "    if largest != i: \n",
    "        arr[i],arr[largest] = arr[largest],arr[i] # swap \n",
    "\n",
    "        # Heapify the root. \n",
    "        heapify(arr, n, largest) \n",
    "\n",
    "# The main function to sort an array of given size \n",
    "def heapSort(arr): \n",
    "    n = len(arr) \n",
    "\n",
    "    # Build a maxheap. \n",
    "    for i in range(n, -1, -1): \n",
    "        heapify(arr, n, i) \n",
    "\n",
    "    # One by one extract elements \n",
    "    for i in range(n-1, 0, -1): \n",
    "        arr[i], arr[0] = arr[0], arr[i] # swap \n",
    "        heapify(arr, i, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for the user\n",
    "while True:\n",
    "    try:\n",
    "        Choice = int(input('which search: 1, 2 or 3?  '))\n",
    "        if Choice == 3:\n",
    "            \n",
    "            Query = input('amazing: the query is?  ')\n",
    "            # maybe somthing as 'do you want specify the director?'\n",
    "            k = input(\"HOW MANY DOCUMENTS YOU WANT?\\nhe defolt value is 3, press enter if you don't have a preference  \")\n",
    "            if k == '':\n",
    "                k = 2\n",
    "            else:\n",
    "                k = int(k)-1\n",
    "            composer = input(\"CHOOSE THE MUSIC COMPOSER. \\nPress enter if you don't have a preference  \")\n",
    "            director = input(\"CHOOSE THE DIRECTOR? \\nPress enter if you don't have a preference  \")\n",
    "            \n",
    "            \n",
    "            pass\n",
    "        else:\n",
    "            Query = input('amazing: the query is?  ')\n",
    "        break\n",
    "    except:\n",
    "        print('try again')\n",
    "\n",
    "# filtering the query\n",
    "Query = FILTER(Query) \n",
    "\n",
    "# initialize two arrays\n",
    "All_docs = []\n",
    "Docs = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if conjuntive query\n",
    "if Choice == 1:\n",
    "    # search all the rigth movies\n",
    "    Films = conjuntive_query(Query, Inverted_index, Vocabulary, Docs)\n",
    "    if len(Films) == 0:\n",
    "        print('Sorry but there is nothing for you, try again')\n",
    "    else:\n",
    "        #make the dataframe\n",
    "        df = output_conjuntive_query(Films, input_path, Doc_Id_url)\n",
    "        print(df)\n",
    "\n",
    "if Choice == 2:\n",
    "    \n",
    "\n",
    "if Choice == 3:\n",
    "    # we star from the conjuntive query\n",
    "    Films = conjuntive_query(Query, Inverted_index, Vocabulary, Docs)\n",
    "    if len(Films) < k:\n",
    "        k = len(Films) - 1\n",
    "    \n",
    "    composer = FILTER(composer)\n",
    "    director = FILTER(director)\n",
    "    \n",
    "    \n",
    "    #lets take the composer and the director\n",
    "    Composers, Directors = search_composer_director(Films, input_path)\n",
    "    \n",
    "    # the score is based on the frequence of the words inside the documents\n",
    "    freq = FREQ_value(Inverted_index, Vocabulary, Films)\n",
    "    \n",
    "    # if the user write sometring inside the director or the composer if the name compare inthe movie the score is \n",
    "    # If the user writes something in the section for the director and for the composer, for each word that matches the document the frequency count is increased by 25%\n",
    "    #this is the counter of the matches\n",
    "    counter_film = COUNT_value()\n",
    "    \n",
    "    # increasing the score\n",
    "    Score_movie = FINAL_SCORE()\n",
    "    if len(Films) != 0:\n",
    "            \n",
    "        Film_score = dict(zip(Score_movie, Films))\n",
    "        \n",
    "        # sort with the heap map\n",
    "        heapSort(Score_movie) \n",
    "        \n",
    "        # save the dataframe\n",
    "        df = output_new_score(Films, input_path, Score_movie, Doc_Id_url, k, Film_score)\n",
    "        print (df)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The Idea that is referred in this question is of the longest palindromic subsequence. I am defining this algorithm with the dynamic programming approach which is defined as dividing a problem into sub-problems, then solving these sub-problems in a way to store their results in a structure dataset for future query. First of all understand the problem, let’s suppose the sequence is “anwaralam” and its length is 9 and index 0-8. So,\n",
    "\n",
    "First part is to identify the subsequences of this sequence, which can be defined as the sub-problem of the main - - problem.\n",
    "\n",
    "\n",
    "Then second part is to identify their palindromic nature from first and last element of the subsequence and store the result in a data structure, (data structure is defined as a 2 dimensional table DST in our case)\n",
    "\n",
    "Thirdly to identify the longest Palindromic Subsequence from the data structure of stored result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Detail ANSWER with Algorithm and Figures is available in Repository FILE: Answer_Algorithm_Question4.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"ANWARALAM\" # defined Sequence\n",
    "nalgo4 = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "## creating a table of rows and column equal to len of string\n",
    "DST= [[0 for i in range(n)]for i in range (n)]\n",
    "print (DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print (nalgo4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# for all values where length of sub sequence is 1\n",
    "for i in range (nalgo4):\n",
    "    DST[i][i] = 1\n",
    "print (DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# for all values where length of subsequence is more than 1\n",
    "# we make a loop for the lengths of sub-sequence    \n",
    "for sseq in range(2, n+1): \n",
    "    for i in range(n-sseq+1):\n",
    "        j = i+sseq-1\n",
    "        if (s[i] == s[j] and sseq == 2): #conditioning for length 2 subsequences\n",
    "            DST[i][j] = 2\n",
    "        elif s[i] == s[j]:  #conditioning for all sseq greater than 2\n",
    "            DST[i][j] = DST[i+1][j-1] + 2\n",
    "        else:\n",
    "            DST[i][j] = max(DST[i][j-1], DST[i+1][j])\n",
    "    print (DST)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 3, 3, 3, 3, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "print(DST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Longest Palindromic sub Sequence is  5\n"
     ]
    }
   ],
   "source": [
    "print(\"The Longest Palindromic sub Sequence is \", DST[0][n-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
