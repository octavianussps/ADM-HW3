{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "Defining the function for data collection from the links of HTML pages, and storing in TSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def PRINT(save_path, counter, webContent):\n",
    "    # We are going to create an .html file into the folder with the page\n",
    "    completeName = os.path.join(save_path, 'article_' + str(counter) + '.html')\n",
    "    with open(completeName, 'w') as out_file:\n",
    "        out_file.write(str(webContent))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Collector file for Crawling the data and fetching fuction from Collector_Utils for Storing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from collector_utils import PRINT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning Variable folder where we want save all the html files, for Counter of File extraction status and emplty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we want save all the html files\n",
    "save_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "\n",
    "\n",
    "# counter for the name of the file\n",
    "counter = 0\n",
    "\n",
    "# a list of all the empty pages\n",
    "empty_file = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a loops with in loop for the following task\n",
    "\n",
    "1.To access all HTML file 1 after another from 1 to 3\n",
    "\n",
    "2.Parsing the links (wiki_links) from these, and converting them into a SOUP\n",
    "\n",
    "3.Reading the WEB content and producing HTML file in HTML Folder\n",
    "\n",
    "4.Extracting the data\n",
    "\n",
    "5.Fectching PRINT Function from Collector_Utils\n",
    "\n",
    "6.Saving the HTML data in an HTMl folder\n",
    "\n",
    "7.Counter Ranging from 1 to 5 Secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A loop for each list of link\n",
    "for i in range(1,4):\n",
    "    \n",
    "    # We are reading the HTML with all the links for the pages of wikipedia\n",
    "    list_of_pages = 'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies' + str(i) + '.html'               \n",
    "    request = requests.get(list_of_pages )\n",
    "    \n",
    "    \n",
    "    # we parse this file \n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    \n",
    "    # we create a list with all the links\n",
    "    wiki_links = soup.select('a')\n",
    "    \n",
    "    N = len(wiki_links)\n",
    "    for j in range(0,N):\n",
    "        try:\n",
    "            print(i,j,counter)\n",
    "            \n",
    "            # read the j-th page of wikipedia in the i-th HTML page of all links\n",
    "            response = urllib.request.urlopen(wiki_links[j].get('href'))\n",
    "            webContent = response.read().decode(response.headers.get_content_charset())\n",
    "            \n",
    "            # this function save this page in an .html file inside the folder at the address save_path \n",
    "            PRINT(save_path +'HTML', counter, webContent)\n",
    "            soup_i = BeautifulSoup(webContent, 'html5lib')\n",
    "            \n",
    "        # lets see the exceptions\n",
    "        except urllib.error.HTTPError as e:\n",
    "            # too many request so sleep for 20min and after save the last page\n",
    "            if e.code == 429:\n",
    "                time.sleep(20*60)\n",
    "                response = urllib.request.urlopen(wiki_links[j].get('href'))\n",
    "                webContent = response.read().decode(response.headers.get_content_charset())\n",
    "                PRINT(save_path +'HTML', counter, webContent) \n",
    "            # page not found so save an html with the write 'NA'\n",
    "            if e.code == 404:\n",
    "                PRINT(save_path + 'HTML', counter, 'NA')\n",
    "                empty_file.append(counter)\n",
    "              \n",
    "        \n",
    "        counter += 1\n",
    "        # andom time sleep between 1 and 5 sec\n",
    "        time.sleep(np.random.randint(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "For parsing we have to extract the INTRO, PLOT and INFOBOX for all related information\n",
    "\n",
    "so for this purpose we made a PARSING_UTILS a function Library for parsing the DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING_UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLOT(tag_i, plot):\n",
    "    # from the <h2> until there is a new <h2> we take all the plots\n",
    "    try:\n",
    "        while True:\n",
    "            if tag_i.name == 'h2':\n",
    "                return plot\n",
    "            if tag_i.name == 'p':\n",
    "                plot.append(tag_i.text[:-2].replace('\\n', ''))\n",
    "            tag_i = tag_i.find_next_sibling()\n",
    "    except: \n",
    "        # it's for pages that have nothing after the polt\n",
    "        pass\n",
    "    return plot\n",
    "\n",
    "def INTRO(all_p_i,plot,intro):\n",
    "    # if plot in empty we try to save the first paragraph, otherwise we save 'NA'\n",
    "    if plot=='NA':\n",
    "        try:\n",
    "            return all_p_i[0][-2]\n",
    "        except:\n",
    "            return 'NA'\n",
    "    #select all the paragraph writen after the plot\n",
    "    else:\n",
    "        for par in all_p_i:\n",
    "            if par.text[:-2] == plot:\n",
    "                break\n",
    "            else:\n",
    "                intro.append(par.text.replace('\\n', ''))\n",
    "    return intro\n",
    "\n",
    "\n",
    "def DICT_INFOBOX(soup_i,result):\n",
    "    # we read the infobox and make a dictionary\n",
    "    table = soup_i.find('table', class_='infobox vevent')\n",
    "    for tr in table.find_all('tr'):\n",
    "        if tr.find('th'):\n",
    "            result[tr.find('th').text] = tr.get_text(strip=True, separator=\" \")[len(tr.find('th').text):]#[m.text for m in tr.find_all('td')]#[len(tr.find('th')):]\n",
    "        else:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def EMPTY_INFOBOX():\n",
    "    # if there is not the infobox we save a dictionary with only 'NA' for each key\n",
    "    a = {'Directed by': 'NA',\n",
    "             'Produced by': 'NA',\n",
    "             'Written by': 'NA',\n",
    "             'Starring': 'NA',\n",
    "             'Music by': 'NA',\n",
    "             'Release date': 'NA',\n",
    "             'Running time': 'NA',\n",
    "             'Country': 'NA',\n",
    "             'Language': 'NA',\n",
    "             'Budget': 'NA'}\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Reading from HTML Folder HTML Files\n",
    "\n",
    "2.Extracting the Required data via fetching the function from PARSING_UTILS\n",
    "\n",
    "3.making TSV Output in a TSV folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from parse_utils import PLOT, INTRO, DICT_INFOBOX, EMPTY_INFOBOX\n",
    "import os.path\n",
    "\n",
    "\n",
    "\n",
    "# read the html files from the HTML folder and save the tvs files inside the folder TSV\n",
    "input_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/HTML'\n",
    "output_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/TSV'\n",
    "\n",
    "\n",
    "# how wiki call the plot section\n",
    "possible_plots = ['#Plot','#Plot_summary', '#Premise']\n",
    "     \n",
    "# how save informatrions into the tsv files                  \n",
    "infos = ['title', 'intro', 'plot' ,'film_name', 'director', 'producer', 'writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget']\n",
    "\n",
    "# what wiki write inside the infobox\n",
    "Name = ['Directed by', 'Produced by', 'Written by', 'Starring', 'Music by', 'Release date', 'Running time', 'Country', 'Language', 'Budget']\n",
    "\n",
    "# the list of all the empty html page\n",
    "b = []\n",
    "\n",
    "\n",
    "# read file from i up to N\n",
    "N = 30000\n",
    "i = 0\n",
    "\n",
    "\n",
    "while i < N:\n",
    "    \n",
    "    # select the html file\n",
    "    completeName_input = os.path.join(input_path, 'article_' + str(i) + '.html')\n",
    "    try:\n",
    "        \n",
    "        # open that file\n",
    "        with open(completeName_input, 'r') as out_file:\n",
    "            page = out_file.read() \n",
    "    \n",
    "        soup_i = BeautifulSoup(page, 'html5lib')\n",
    "        \n",
    "        # read all the paragraph\n",
    "        all_p_i = soup_i.find_all('p')\n",
    "       \n",
    "        \n",
    "        \n",
    "        # for each possible name of the plot we do the follow loop\n",
    "        no_plot = 0\n",
    "        for k in possible_plots:\n",
    "            try:\n",
    "                # we are going to select the <h2> associated to the k-th name of the plot\n",
    "                # if we find it we create the plot with the function PLOT inside collectop_utils.py\n",
    "                tag_i = soup_i.select_one(k).find_parent('h2').find_next_sibling()\n",
    "                plot = PLOT(tag_i, [])\n",
    "                no_plot = 1\n",
    "                break                         \n",
    "            except:\n",
    "                pass\n",
    "        # if there is no plot we write 'NA'\n",
    "        if no_plot == 0:\n",
    "            plot.append('NA')\n",
    "        \n",
    "        # reading the intro: everything before the plot\n",
    "        intro = INTRO(all_p_i,plot,[])    \n",
    "        \n",
    "        # try to read the infobox\n",
    "        try: \n",
    "            # if we find it whe read it and save it as a dictionary into the result\n",
    "            table = soup_i.find('table', class_='infobox vevent')\n",
    "            result = DICT_INFOBOX(soup_i,{})\n",
    "        except:\n",
    "            # otherwise we save a dictionary with only 'NA' for each key\n",
    "            result = EMPTY_INFOBOX()\n",
    "        \n",
    "        \n",
    "        # we are going to create an array with only 0\n",
    "        InfoFilm= [0]*14\n",
    "        \n",
    "        # saving the infobox\n",
    "        for k in range(10):\n",
    "            InfoFilm[k+4] = result.get(Name[k], 'NA')\n",
    "        # saving the title\n",
    "        InfoFilm[0] = soup_i.title.text[:-12]\n",
    "        \n",
    "        # saving the film_name\n",
    "        try:\n",
    "            InfoFilm[3] = table.find_all('tr')[0].text\n",
    "        except:\n",
    "            InfoFilm[3] = 'NA'\n",
    "        \n",
    "        # saving plot and intro\n",
    "        InfoFilm[2] = ' '.join(plot)\n",
    "        InfoFilm[1] = ' '.join(intro)\n",
    "        \n",
    "        # save the tsv file\n",
    "        completeName_output = os.path.join(output_path, 'article_' + str(i) + '.tsv')\n",
    "        with open(completeName_output, 'wt') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(InfoFilm)\n",
    "    except:\n",
    "        # if the html in empty, this mean that in collector there was the exception 404 so we save only 'NA' for each informations\n",
    "        InfoFilm = ['NA']*14\n",
    "        completeName_output = os.path.join(output_path, 'article_' + str(i) + '.tsv')\n",
    "        with open(completeName_output, 'wt') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(InfoFilm)\n",
    "        b.append(i)\n",
    "\n",
    "    \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Cleaning of Data\n",
    "Tokenizing | Removal of Stop Words | Stemming\n",
    "Using nltk library\n",
    "\n",
    "#### 2.Indexing of Data\n",
    "Indexing and INVERTED indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# generate the stop-words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# this function work for a generic string, such as a document or the query\n",
    "def FILTER(string):\n",
    "    # we are going to filter the string\n",
    "    string = string.lower()\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    # tokenize the string\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    Vector = tokenizer.tokenize(string)\n",
    "    # removing the stop-words\n",
    "    wordsFiltered = []\n",
    "    for w in Vector:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #stemmer\n",
    "    porter = PorterStemmer()\n",
    "    ListOfWords = [porter.stem(word) for word in wordsFiltered]\n",
    "    return ListOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Creating Direct INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to create the direct index\n",
    "# is a dictionry with as key the word and as value another dictionary with key the number of the document and as value the frequency of the word in this document\n",
    "def DOC_DOCLEN_FREQ_DIC(Document, direct_index, i):\n",
    "    Document = Counter(Document)\n",
    "    for word, frequence in Document.items():\n",
    "        if word not in direct_index:\n",
    "            direct_index[word] = {i: frequence}\n",
    "        else:\n",
    "            direct_index[word][i] = frequence\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Creating INVERTED INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to create the inverted index \n",
    "def INVERTED_INDEX_DIC(direct_index, Inverted_index, Length_Doc, N):\n",
    "    for key, Doc_Freq in direct_index.items():\n",
    "        Inverted_index[key] = {}\n",
    "        for doc, freq in Doc_Freq.items():\n",
    "            if key not in Inverted_index:\n",
    "                Inverted_index[key] = {doc: (freq/ Length_Doc[doc])*np.log(N/len(Doc_Freq))}\n",
    "            else:\n",
    "                Inverted_index[key][doc] = (freq/ Length_Doc[doc])*np.log(N/len(Doc_Freq))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all these three functions in INDEX_UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# generate the stop-words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# this function work for a generic string, such as a document or the query\n",
    "def FILTER(string):\n",
    "    # we are going to filter the string\n",
    "    string = string.lower()\n",
    "    string = string.replace(\"'\", \"\")\n",
    "    # tokenize the string\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    Vector = tokenizer.tokenize(string)\n",
    "    # removing the stop-words\n",
    "    wordsFiltered = []\n",
    "    for w in Vector:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #stemmer\n",
    "    porter = PorterStemmer()\n",
    "    ListOfWords = [porter.stem(word) for word in wordsFiltered]\n",
    "    return ListOfWords\n",
    "\n",
    "# we use this function to create the direct index\n",
    "# is a dictionry with as key the word and as value another dictionary with key the number of the document and as value the frequency of the word in this document\n",
    "def DOC_DOCLEN_FREQ_DIC(Document, direct_index, i):\n",
    "    Document = Counter(Document)\n",
    "    for word, frequence in Document.items():\n",
    "        if word not in direct_index:\n",
    "            direct_index[word] = {i: frequence}\n",
    "        else:\n",
    "            direct_index[word][i] = frequence\n",
    "    return \n",
    "\n",
    "# we use this function to create the inverted index \n",
    "def INVERTED_INDEX_DIC(direct_index, Inverted_index, Length_Doc, N):\n",
    "    for key, Doc_Freq in direct_index.items():\n",
    "        Inverted_index[key] = {}\n",
    "        for doc, freq in Doc_Freq.items():\n",
    "            if key not in Inverted_index:\n",
    "                Inverted_index[key] = {doc: (freq/ Length_Doc[doc])*np.log(N/len(Doc_Freq))}\n",
    "            else:\n",
    "                Inverted_index[key][doc] = (freq/ Length_Doc[doc])*np.log(N/len(Doc_Freq))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "Reading Data from TSV file\n",
    "\n",
    "Creating Vocablury\n",
    "\n",
    "Saving the Dictionary and Inverted INDEX as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from index_utils import FILTER, DOC_DOCLEN_FREQ_DIC, INVERTED_INDEX_DIC\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# we read the tsv files from the folder TSV \n",
    "output_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/TSV'\n",
    "\n",
    "\n",
    "## there are dictionary of dictionary because is easy to read\n",
    "'''\n",
    "{ word_id : {doc_id : tfidf,\n",
    "             doc_id : tfidf,\n",
    "             doc_id : tfidf }, \n",
    "  word _id: {doc_id : tfidf,\n",
    "             doc_id : tfidf,\n",
    "             doc_id : tfidf,\n",
    "             doc_id : tfidf}\n",
    "  }\n",
    "'''\n",
    "\n",
    "# is a dictionry with as key the word and as value another dictionary with key the number of the document and as value the frequency of the word in this document\n",
    "direct_index = {}\n",
    "\n",
    "#the inverted index with as key the words\n",
    "Inverted_index_words = {}\n",
    "\n",
    "#the inverted index with as key the words_number read from the Vocabulary\n",
    "Inverted_index = {}\n",
    "\n",
    "# as key the word as value the number\n",
    "Vocabulary = {}\n",
    "\n",
    "# as key the number of the document as value his length after cleaning\n",
    "Length_Doc = {}\n",
    "\n",
    "N = 30000\n",
    "i = 0\n",
    "while i < N:\n",
    "    \n",
    "    # read the i-th tsv\n",
    "    completeName_output = os.path.join(output_path, 'article_' + str(i) + '.tsv')\n",
    "    with open(completeName_output, 'r') as input_file:\n",
    "        intro_plot = input_file.readline()\n",
    "    # select only the intro and the plot\n",
    "    intro_plot = intro_plot.split('\\t')[1:3]\n",
    "    # make a string\n",
    "    intro_plot = str(intro_plot[0]) + ' ' + str(intro_plot[1])\n",
    "    # filter this string\n",
    "    Document = FILTER(intro_plot)\n",
    "    # save the length of the document\n",
    "    Length_Doc[i] = len(Document) \n",
    "    \n",
    "    # make the direct index\n",
    "    DOC_DOCLEN_FREQ_DIC(Document, direct_index, i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# save the inverted index with the words as key\n",
    "INVERTED_INDEX_DIC(direct_index, Inverted_index_words, Length_Doc, N)\n",
    "\n",
    "# create the Vocabulary and the real inverted index\n",
    "counter = 0\n",
    "for i,j in Inverted_index_words.items():\n",
    "    Vocabulary[i] = counter\n",
    "    Inverted_index[counter] = j\n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "file = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "\n",
    "# save the 3 dictionary as pkl\n",
    "with open(file+'Inverted_index.pkl', 'wb') as fp:\n",
    "    pickle.dump(Inverted_index, fp, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(file+'Vocabulary.pkl', 'wb') as fp:\n",
    "    pickle.dump(Vocabulary, fp, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(file+'Length_doc.pkl', 'wb') as fp:\n",
    "    pickle.dump(Length_Doc, fp, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCH Engine 1\n",
    "### Conjunctive Query\n",
    "Create a dictionary with links and number in Collectro.py\n",
    "\n",
    "Generate a dataframe\n",
    "\n",
    "re-upload index and index_utils collector pars\n",
    "\n",
    "### INPUT and EXECUTE query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from index_utils import FILTER\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "\n",
    "\n",
    "def conjuntive_query(Query, Inverted_index, Vocabulary, Docs):\n",
    "    for i in Query:\n",
    "        # select all the docs with a specific word and appen this array to another array\n",
    "        Docs.append(np.fromiter(Inverted_index[Vocabulary[i]].keys(), dtype=int))\n",
    "    # make the and\n",
    "    Films = reduce(np.intersect1d, Docs)\n",
    "    return Films\n",
    "\n",
    "\n",
    "\n",
    "def output_conjuntive_query(Films, input_path):\n",
    "    Title = []\n",
    "    Intro = []\n",
    "    Url = []\n",
    "    # for each film selected we are going to save title, intro and url\n",
    "    for i in Films:\n",
    "        completeName_input = os.path.join(input_path, 'article_' + str(i) + '.tsv')\n",
    "        with open(completeName_input, 'r') as input_file:\n",
    "            document = input_file.readline()\n",
    "        document= document.split('\\t')\n",
    "        Title.append(document[0])\n",
    "        Intro.append(document[1])\n",
    "        Url.append(Doc_Id_url[i])\n",
    "        \n",
    "    #create a dictionary\n",
    "    Data = {'title' : Title,\n",
    "            'intro' : Intro,\n",
    "            'url' : Url\n",
    "            }\n",
    "    # make the dataframe \n",
    "    df = pd.DataFrame(data=Data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# folder with the tsv files\n",
    "input_path = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/TSV'\n",
    "\n",
    "# reading the dictionaries\n",
    "file = '/home/lex/Desktop/Data_science/Algorithmic_Methods_of_Data_Mining/ADM_hw3/'\n",
    "with open(file + 'Inverted_index.pkl', 'rb') as fp:\n",
    "    Inverted_index = pickle.load(fp)\n",
    "\n",
    "with open(file + 'Vocabulary.pkl', 'rb') as fp:\n",
    "    Vocabulary = pickle.load(fp)\n",
    "\n",
    "with open(file + 'Doc_Id_url.pkl', 'rb') as fp:\n",
    "    Doc_Id_url = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "# input for the user\n",
    "while True:\n",
    "    try:\n",
    "        Choice = int(input('which search: 1, 2 or 3?'))\n",
    "        if Choice == 3:\n",
    "            # maybe somthing as 'do you want specify the director?'\n",
    "            pass\n",
    "        else:\n",
    "            Query = input('amazing: the query is?')\n",
    "        break\n",
    "    except:\n",
    "        print('try again')\n",
    "\n",
    "# filtering the query\n",
    "Query = FILTER(Query) \n",
    "\n",
    "# initialize two arrays\n",
    "All_docs = []\n",
    "Docs = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if conjuntive query\n",
    "if Choice == 1:\n",
    "    # search all the rigth movies\n",
    "    Films = conjuntive_query(Query, Inverted_index, Vocabulary, Docs)\n",
    "    #make the dataframe\n",
    "    df = output_conjuntive_query(Films, input_path)\n",
    "    print(df)\n",
    "\n",
    "if Choice == 2:\n",
    "    pass\n",
    "\n",
    "if Choice == 3:\n",
    "    pass\n",
    "\n",
    "\n",
    "'''\n",
    "for i in Films:\n",
    "    print(Doc_Id_url[i])\n",
    "c=[]\n",
    "for i in Vocabulary:\n",
    "    c.append(i)    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCH Engine  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCH Engine  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The Idea that is referred in this question is of the longest palindromic subsequence. I am defining this algorithm with the dynamic programming approach which is defined as dividing a problem into sub-problems, then solving these sub-problems in a way to store their results in a structure dataset for future query. First of all understand the problem, let’s suppose the sequence is “anwaralam” and its length is 9 and index 0-8. So,\n",
    "\n",
    "First part is to identify the subsequences of this sequence, which can be defined as the sub-problem of the main - - problem.\n",
    "\n",
    "\n",
    "Then second part is to identify their palindromic nature from first and last element of the subsequence and store the result in a data structure, (data structure is defined as a 2 dimensional table DST in our case)\n",
    "\n",
    "Thirdly to identify the longest Palindromic Subsequence from the data structure of stored result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Detail ANSWER with Algorithm and Figures is available in Repository FILE: Answer_Algorithm_Question4.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"ANWARALAM\" # defined Sequence\n",
    "nalgo4 = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "## creating a table of rows and column equal to len of string\n",
    "DST= [[0 for i in range(n)]for i in range (n)]\n",
    "print (DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print (nalgo4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# for all values where length of sub sequence is 1\n",
    "for i in range (nalgo4):\n",
    "    DST[i][i] = 1\n",
    "print (DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "[[1, 1, 1, 3, 3, 3, 3, 5, 5], [0, 1, 1, 1, 1, 3, 3, 3, 3], [0, 0, 1, 1, 1, 3, 3, 3, 3], [0, 0, 0, 1, 1, 3, 3, 3, 3], [0, 0, 0, 0, 1, 1, 1, 3, 3], [0, 0, 0, 0, 0, 1, 1, 3, 3], [0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# for all values where length of subsequence is more than 1\n",
    "# we make a loop for the lengths of sub-sequence    \n",
    "for sseq in range(2, n+1): \n",
    "    for i in range(n-sseq+1):\n",
    "        j = i+sseq-1\n",
    "        if (s[i] == s[j] and sseq == 2): #conditioning for length 2 subsequences\n",
    "            DST[i][j] = 2\n",
    "        elif s[i] == s[j]:  #conditioning for all sseq greater than 2\n",
    "            DST[i][j] = DST[i+1][j-1] + 2\n",
    "        else:\n",
    "            DST[i][j] = max(DST[i][j-1], DST[i+1][j])\n",
    "    print (DST)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 3, 3, 3, 3, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "print(DST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Longest Palindromic sub Sequence is  5\n"
     ]
    }
   ],
   "source": [
    "print(\"The Longest Palindromic sub Sequence is \", DST[0][n-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
